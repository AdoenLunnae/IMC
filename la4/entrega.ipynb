{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB ASSIGNMENT 4: SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Computational Models (Dpto. de Informática y Análisis Numérico) -- EPSC (UCO)\n",
    "## Academic year 2020/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Álvaro Prieto Barón (i72prbaa@uco.es) DNI: 49506913B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import libsvm_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Explain the contents and result of `libsvm.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains the code needed to train an SVM and plot its result. It uses a lineal kernel, and has a regularization strenght proportional to 1/1000 (as it is inversely proportional to the `C` parameter).\n",
    "\n",
    "To better control the parameters of the calls, we create a function in another file(`libsvm_param.py` that runs a parametrized version of the same code.\n",
    "\n",
    "When we run it, we obtain the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "libsvm_param.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, we can see several things:\n",
    "- The red and blue dots are the patterns of the dataset, with the color representing the class.\n",
    "- The points marked with a `+` are the support vectors of the model.\n",
    "- The lines are the separating hyperplane(continuous) and margin(dotted)\n",
    "- The colored areas are the regions associated with each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Which hyperplane will make less mistake(intuitively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An hyperplane that passes close to the two points that are separated from the rest, even if it misclassifies one of them as they are probably outliers or noise, so it gets better at generalising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Use different values of C and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exp in range (-2, 5):\n",
    "    libsvm_param.run(c=10**exp, fig_name=f\"C=10^{exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for very low values of C, the model tries to have a very big margin, making a lot of mistakes in the process, while for very high values, it is more accurate, but has smaller margins. The best value for C is around 100, which produces no errors and has a bigger margin. Lower values have too many errors, and higher ones have practically the same accuracy but less margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Do the same for the second dataset and comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exp in range (-2, 5):\n",
    "    libsvm_param.run(c=10**exp, fig_name=f\"C=10^{exp}\", dataset=\"Datasets/csv/dataset2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't get any particularly good results, as the data is not linearly separable in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Proposing a solution for dataset2 with a Gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for c_exp in range (-2, 5):\n",
    "    for g_exp in range (-2, 5):\n",
    "        libsvm_param.run_gaussian(c=10**c_exp, gamma=10**g_exp,fig_name=f\"C=10^{c_exp}, gamma=10^{g_exp}\", dataset=\"Datasets/csv/dataset2.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above runs a new function, called `run_gaussian` to train and plot SVMs with all the possible combinations of C and $\\gamma$. After running it (full output not included to save space), we can confirm that the best possible configuration is C $=10^4$, $\\gamma=10$. Its output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "libsvm_param.run_gaussian(c=10**4, gamma=10, fig_name=\"Correct classification of dataset 2\", dataset=\"Datasets/csv/dataset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check some of the other configurations, some of them produce overfitting, like C $=10^4$, $\\gamma=10^3$ (mild) and C $=10^4$, $\\gamma=10^4$ (severe). In the severe case, we can observe that the SVM learns the red patterns so well that the only area it assigns to them is almost a single point. This is because for large values of $\\gamma$, the radius becomes smaller and smaller(in the case of $\\gamma=10^4$, $r\\approx.007$), which produces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "libsvm_param.run_gaussian(c=10**4, gamma=10**3, fig_name=\"Mild overfitting in dataset 2\", dataset=\"Datasets/csv/dataset2.csv\")\n",
    "libsvm_param.run_gaussian(c=10**4, gamma=10**4, fig_name=\"Severe overfitting in dataset 2\", dataset=\"Datasets/csv/dataset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand we can check some underfitting cases too. We will ignore the cases where the data is not divided (C or $\\gamma$ too low), and see some examples of mild(C$=10^2$, $\\gamma=10$), medium(C $=10$, $\\gamma=10$) and severe(C $=10$, $\\gamma=1$) underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "libsvm_param.run_gaussian(c=10**2, gamma=10, fig_name=\"Mild underfitting in dataset 2\", dataset=\"Datasets/csv/dataset2.csv\")\n",
    "libsvm_param.run_gaussian(c=10, gamma=10, fig_name=\"Medium underfitting in dataset 2\", dataset=\"Datasets/csv/dataset2.csv\")\n",
    "libsvm_param.run_gaussian(c=10, gamma=1, fig_name=\"Severe underfitting in dataset 2\", dataset=\"Datasets/csv/dataset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Is dataset 3 linearly separable? Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.show_data('Datasets/csv/dataset3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not linearly separable, it is easy to see the 2 outliers, as they are red points in the middle of the blue ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:  Run a non-linear SVM and get the best values for C and $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run again the code that tests all the configurations, and find that the best fit is obtained with the parameters C $=1$ and $\\gamma=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.run_gaussian(c=1, gamma=1, dataset='Datasets/csv/dataset3.csv', fig_name=\"Correct classification of dataset 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can select some cases of mild(C $=10^2$, $\\gamma=10$), moderate(C $=10^3$, $\\gamma=10$) and severe(C $=10^3$, $\\gamma=10^3$) overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.run_gaussian(c=10**2, gamma=10, fig_name=\"Mild overfitting in dataset 3\", dataset=\"Datasets/csv/dataset3.csv\")\n",
    "libsvm_param.run_gaussian(c=10**3, gamma=10, fig_name=\"Moderate overfitting in dataset 3\", dataset=\"Datasets/csv/dataset3.csv\")\n",
    "libsvm_param.run_gaussian(c=10**3, gamma=10**3, fig_name=\"Severe overfitting in dataset 3\", dataset=\"Datasets/csv/dataset3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for underfitting cases, where we will visualize two of them, mild(C $=10$, $\\gamma=10^-1$) and severe(C $=1$, $\\gamma=10^-1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.run_gaussian(c=10, gamma=10**-1, fig_name=\"Mild underfitting in dataset 3\", dataset=\"Datasets/csv/dataset3.csv\")\n",
    "libsvm_param.run_gaussian(c=1, gamma=10**-1, fig_name=\"Severe underfitting in dataset 3\", dataset=\"Datasets/csv/dataset3.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Make a full training and prediction proccess using dataset3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, model_selection, svm\n",
    "\n",
    "dataset3='Datasets/csv/dataset3.csv'\n",
    "\n",
    "data = pd.read_csv(dataset3, header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the split and scaling\n",
    "\n",
    "We will use `1` as the seed at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with all possible configurations\n",
    "We will generate a list of all the configurations and iterate over it, doing the training process and saving the accuracy in another list. After that, we can check the index of the best accuracy and select that configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [(c_exp,g_exp) for c_exp in range(-2,5) for g_exp in range(-2,5)]\n",
    "acc_linear = [] \n",
    "acc_gaussian = []\n",
    "for c_exp, g_exp in configs:\n",
    "    linear_model = svm.SVC(kernel='linear', C=10**c_exp)\n",
    "    rbf_model = svm.SVC(C=10**c_exp, gamma=10**g_exp)\n",
    "\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    rbf_model.fit(X_train, y_train)\n",
    "    \n",
    "    acc_linear.append(linear_model.score(X_test, y_test))\n",
    "    acc_gaussian.append(rbf_model.score(X_test, y_test))\n",
    "    \n",
    "best_linear_config = configs[np.argmax(acc_linear)]\n",
    "best_linear_score = acc_linear[np.argmax(acc_linear)]\n",
    "\n",
    "best_gaussian_config = configs[np.argmax(acc_gaussian)]\n",
    "best_gaussian_score = acc_gaussian[np.argmax(acc_gaussian)]\n",
    "\n",
    "print(f\"Best linear configuration: C=10^{best_linear_config[0]} - Score: {best_linear_score}\")\n",
    "print(f\"Best gaussian configuration: C=10^{best_gaussian_config[0]}, gamma=10^{best_gaussian_config[1]} - Score: {best_gaussian_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test it with different seeds, we can parametrize it and make another function, `split_and_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.split_and_train(dataset3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.split_and_train(dataset3, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.split_and_train(dataset3, 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this examples we can appreciate how sensitive the model is to changes in the training set, resulting in different parameters for each seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: Parameter optimization using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "dataset3='Datasets/csv/dataset3.csv'\n",
    "\n",
    "data = pd.read_csv(dataset3, header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=35)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "Cs = np.logspace(-4, 4, num=9, base=10)\n",
    "Gs = np.logspace(-4, 4, num=9, base=10)\n",
    "\n",
    "optimal=GridSearchCV(estimator=svm_model, param_grid=dict(C=Cs,gamma=Gs), n_jobs=2, cv=5)\n",
    "\n",
    "optimal.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "chosen_c=optimal.best_estimator_.C\n",
    "chosen_gamma=optimal.best_estimator_.gamma\n",
    "\n",
    "print(f\"Best estimator: C={chosen_c}, gamma={chosen_gamma}\")\n",
    "\n",
    "print(f\"Accuracy: Train->{optimal.score(X_train, y_train)*100}% Test->{optimal.score(X_test, y_test)*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chosen seed (`35`), we can see that the parameters change a bit, but the accuracy is practically the same. This method has the advantage of being simpler and performing cross-validation automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: Drawbacks of doing the optimization manually using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are using the test set to choose the better model, which makes it less valuable in terms of information, and makes the model worst at generalizing with new data. Furthermore, adjusting the parameters by hand it is easier to leave out a better configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11: Implement K-fold nested cross validation without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "dataset3='Datasets/csv/dataset3.csv'\n",
    "\n",
    "data = pd.read_csv(dataset3, header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=35)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "folds = list(zip(*[fold for fold in kfold.split(X_train, y_train)]))\n",
    "\n",
    "train_sets, test_sets = folds[0], folds[1]\n",
    "\n",
    "Cs = np.logspace(-4, 4, num=9, base=10)\n",
    "Gs = np.logspace(-4, 4, num=9, base=10)\n",
    "\n",
    "average_scores = np.zeros((len(Cs),len(Gs)))\n",
    "\n",
    "for c_index, C in enumerate(Cs):\n",
    "    for g_index, gamma in enumerate(Gs): #Outer loop \n",
    "        for train_set, test_set in zip(train_sets, test_sets): #Inner loop\n",
    "            model = svm.SVC(C=C, gamma=gamma)\n",
    "            model.fit([X_train[i] for i in train_set], [y_train[i] for i in train_set])\n",
    "            \n",
    "            average_scores[c_index, g_index] += model.score([X_train[i] for i in test_set], [y_train[i] for i in test_set])\n",
    "        average_scores[c_index, g_index] /= 5\n",
    "\n",
    "best_config = np.unravel_index(np.argmax(average_scores), average_scores.shape)\n",
    "\n",
    "best_c = Cs[best_config[0]]\n",
    "best_gamma = Gs[best_config[1]]\n",
    "\n",
    "best_score = average_scores[best_config[0], best_config[1]]\n",
    "\n",
    "print(f\"Best configuration => C={best_c}, gamma={best_gamma}.\\nTrain accuracy = {best_score*100}%\")\n",
    "\n",
    "model = svm.SVC(C=best_c, gamma=best_gamma)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test accuracy = {model.score(X_test, y_test)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We get the same parameters as we did when using GridSearchCV, although there are a few other combinations that give the maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12: Applying the script from question 9 to the noMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we parametrize the script and add it to the `libsvm_param` file. We can then run it with the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.cross_validation_search('Datasets/csv/train_nomnist.csv', 'Datasets/csv/test_nomnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13: Measure time used with K=3, K=5 and K=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `timeit` module to measure de computational time. We'll also add a parameter to the `cross_validation_search` to control the number of folds (parameter `cv` in the GridSearchCV.) We could also pass a different set of values for C or $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "for x in [3, 5, 10]:\n",
    "    print(timeit.timeit(f\"import libsvm_param;libsvm_param.cross_validation_search('Datasets/csv/train_nomnist.csv', 'Datasets/csv/test_nomnist.csv', folds={x})\", number=1), \" seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the execution times are around 1 minute for K $=3$, around 2 for K $=5$ and around 4 for K $=10$. This is understandable, as the number of trainings is proportional to the number of folds, and their execution times are extremely similar.\n",
    "\n",
    "In this case, the accuracy does not improve with the number of folds, so the 3-fold is a good option, as it saves computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14: Train a linear model for the spam dataset and get the best value for C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a function similar to the one in question 9, that instead of using a gaussian kernel, uses a linear one. W can call it and obtain the best value for the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_param.cross_validation_search_linear('Datasets/csv/train_spam.csv', 'Datasets/csv/test_spam.csv', folds=3, c_vals=[.001, .01, .1, 1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15: Construct the confusion matrix and analize the misclassified patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the datasets (there is no need to scale, as they contain only binary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Datasets/csv/train_spam.csv', header=None)\n",
    "test_data = pd.read_csv('Datasets/csv/test_spam.csv', header=None)\n",
    "\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values\n",
    "\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a model with C $=0.01$ and construct the confusion matrix. We need the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "model = svm.SVC(kernel='linear', C=0.01)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 safe emails classified as spam and 6 spam emails classified as safe. We will check which words are in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "false_positives=[pattern for i, pattern in enumerate(X_test) if (y_pred[i]==1 and y_test[i]==0)]\n",
    "words_in_fp = np.zeros((1899,))\n",
    "\n",
    "for pattern in false_positives:\n",
    "    words = np.where(pattern)[0]\n",
    "    for word in words:\n",
    "        words_in_fp[word] += 1\n",
    "        \n",
    "flagged_words = np.where(words_in_fp > 9)[0]\n",
    "\n",
    "vocab = pd.read_csv('vocab.txt', delimiter='\\t', dtype='str', usecols=[1])\n",
    "vocab = vocab.iloc[:,:].values\n",
    "\n",
    "for flagged in flagged_words:\n",
    "    print(vocab[flagged])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the words that are in at least 10 of the 11  false positives, whe can conclude that some of them are likely responsible for the misclassification, for example, 'island', which is not a normal term to include in a mail, 'off' which normally appears when talking about offers, or 'onc'(once) and 'todai', which are also common in spam. We could also compare their frequencies in positive patterns and in the whole dataset, to see which ones are actually more frequent in spam mails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16: Train an RBF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the script from assignment 3 (`rbf.py`) and determine that the best seed is 3. To show its result, we create a modified version that has no CLI and runs only a fixed seed. We call it with the default arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rbf_1_seed\n",
    "\n",
    "rbf_1_seed.train_rbf_total(\n",
    "    'Datasets/csv/train_spam.csv', 'Datasets/csv/test_spam.csv', classification=True,\n",
    "    ratio_rbf=.1, l2=False, eta=1e-2, outputs=1, model=\"\", pred=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a test CCR slightly higher than with the SVM while the training CCR is a bit lower. This means that the RBF is generalising better, but the difference is negictible, and SVM has less meta-parameters to tune (C and $\\gamma$, while an RBF network has the number of RBF neurons, the regularization type and $\\eta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17: Train a non-linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use again the `cross_validation_search` function to find the best parameters. We will run it with values of C $\\in\\{0.01, 0.1, 1, 10, 100\\}$ and of $\\gamma\\in\\{0.01, 0.1, 1, 10, 100\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2, 2, 5)\n",
    "Gs = np.logspace(-2, 2, 5)\n",
    "\n",
    "libsvm_param.cross_validation_search('Datasets/csv/train_spam.csv', 'Datasets/csv/test_spam.csv', c_vals=Cs, gamma_vals=Gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linear classifier has and even higher CCR on the training set, but a much worse performance on the test set, suggesting that it suffers from overfitting even with the largest radius. We allow smaller values of $\\gamma$ to try to avoid it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2, 2, 5)\n",
    "Gs = np.logspace(-4, 0, 5)\n",
    "\n",
    "libsvm_param.cross_validation_search('Datasets/csv/train_spam.csv', 'Datasets/csv/test_spam.csv', c_vals=Cs, gamma_vals=Gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the same test result as with the RBF network, with a slight increase in training accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
